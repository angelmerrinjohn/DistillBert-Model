# DistillBert-Model
 DistilBERT, an efficient, distilled version of BERT. This empowers users to train their own smaller and faster language models on custom datasets, making them ideal for performance-critical applications.
